{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c/miniconda3/envs/cs285/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cpu = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "weights_path = './vicuna_weight.h5'\n",
    "\n",
    "weights = []\n",
    "w_input = []\n",
    "attn_weights = []\n",
    "aw_input = []\n",
    "q_weights = []\n",
    "k_weights = []\n",
    "\n",
    "with h5py.File(weights_path, 'r') as weight_file:\n",
    "    for layer_name in weight_file:\n",
    "        w = np.squeeze(np.array(weight_file[layer_name])).astype(np.float32)\n",
    "        if \"model\" in layer_name and \"embed_tokens\" not in layer_name and \"layernorm\" not in layer_name:\n",
    "            weights.append(w)\n",
    "            w_input.append(rng.random(w.shape, dtype = np.float32))\n",
    "        if \"attn\" in layer_name:\n",
    "            attn_weights.append(w)\n",
    "            aw_input.append(rng.random(w.shape[1], dtype = np.float32))\n",
    "            if \"q_proj\" in layer_name:\n",
    "                q_weights.append(w)\n",
    "            if \"k_proj\" in layer_name:\n",
    "                k_weights.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(input1, input2, f, runner):\n",
    "    runs = 10\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        times.append(runner(input1, input2, f))\n",
    "    times = np.array(times)\n",
    "    print(f\"{runner.__name__[:-6]}pytorch_with_load\")\n",
    "    print(f\"{np.average(times)}ms +/- {np.std(times)}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elewise_mul_torch(input1, input2, hidden_dim):\n",
    "    return torch.multiply(input1[:hidden_dim], input2[:hidden_dim])\n",
    "\n",
    "def elewise_mul_runner(inputs1, inputs2, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs1)):\n",
    "        input1 = inputs1[i].flatten()\n",
    "        input2 = inputs2[i].flatten()\n",
    "        hd = len(input1)\n",
    "\n",
    "        inp1 = torch.from_numpy(input1).to(dtype=torch.float32).to(cpu)\n",
    "        inp2 = torch.from_numpy(input2).to(dtype=torch.float32).to(cpu)\n",
    "        hidden_dim = torch.tensor(hd).to(cpu)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        inp1 = inp1.to(device)\n",
    "        inp2 = inp2.to(device)\n",
    "        hidden_dim = hidden_dim.to(device)\n",
    "        res = elewise_mul_torch(inp1, inp2, hidden_dim)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        del inp2\n",
    "        del inp1\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_torch(weight, input):\n",
    "    return torch.matmul(weight, input)\n",
    "\n",
    "def matmul_runner(weights, inputs, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        weight = weights[i]\n",
    "        input = inputs[i]\n",
    "\n",
    "        w = torch.from_numpy(weight).to(dtype=torch.float32).to(cpu)\n",
    "        inp = torch.from_numpy(input).to(dtype=torch.float32).to(cpu)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        w = w.to(device)\n",
    "        inp = inp.to(device)\n",
    "        res = matmul_torch(w, inp)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        del inp\n",
    "        del w\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiquery_attention_part1_torch(token_position, head, head_size, key_cache_layer, q):\n",
    "    return torch.matmul(key_cache_layer[:token_position][:, (head * head_size):(head * head_size) + head_size], q[(head * head_size):(head * head_size) + head_size])/ torch.sqrt(torch.as_tensor(head_size * 1))\n",
    "\n",
    "def multiquery_attention_part1_runner(k_matrixes, q_matrixes, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(k_matrixes)):\n",
    "        k_matrix = k_matrixes[i]\n",
    "        q_matrix = q_matrixes[i]\n",
    "        token_position = k_matrix.shape[0] - 1\n",
    "\n",
    "        num_head = 32\n",
    "        head = int(rng.integers(low=0, high=num_head))\n",
    "        head_size = k_matrix.shape[0] // num_head\n",
    "        \n",
    "        key_cache_layer = torch.from_numpy(k_matrix).to(cpu)\n",
    "        q = torch.from_numpy(q_matrix.flatten()).to(cpu)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        key_cache_layer = key_cache_layer.to(device)\n",
    "        q = q.to(device)\n",
    "        res = multiquery_attention_part1_torch(token_position, head, head_size, key_cache_layer, q)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        del key_cache_layer\n",
    "        del q\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiquery_attention_part2_torch(token_position, head, head_size, key_cache_layer, attention):\n",
    "    return torch.matmul(torch.transpose(key_cache_layer[:token_position + 1][:, (head * head_size):(head * head_size) + head_size], 0, 1), attention[:token_position + 1])\n",
    "\n",
    "def multiquery_attention_part2_runner(k_matrixes, q_matrixes, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(k_matrixes)):\n",
    "        k_matrix = k_matrixes[i]\n",
    "        q_matrix = q_matrixes[i]\n",
    "        token_position = k_matrix.shape[0] - 1\n",
    "\n",
    "        num_head = 32\n",
    "        head = int(rng.integers(low=0, high=num_head))\n",
    "        head_size = k_matrix.shape[0] // num_head\n",
    "        \n",
    "        key_cache_layer = torch.from_numpy(k_matrix).to(cpu)\n",
    "        q = torch.from_numpy(q_matrix.flatten()).to(cpu)\n",
    "\n",
    "        attention = multiquery_attention_part1_torch(token_position, head, head_size, key_cache_layer, q).to(cpu)\n",
    "        attention = torch.cat((attention, torch.tensor([0]).to(cpu))).to(cpu)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        key_cache_layer = key_cache_layer.to(device)\n",
    "        attention = attention.to(device)\n",
    "        res = multiquery_attention_part2_torch(token_position, head, head_size, key_cache_layer, attention)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        del key_cache_layer\n",
    "        del attention\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm_part1_torch(input, weight):\n",
    "    return torch.sum(torch.multiply(input, input))\n",
    "\n",
    "def rmsnorm_part1_runner(weights, inputs, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i].flatten()\n",
    "        weight = weights[i].flatten()\n",
    "        \n",
    "        inp = torch.from_numpy(input).to(dtype=torch.float32).to(cpu)\n",
    "        w = torch.from_numpy(weight).to(dtype=torch.float32).to(cpu)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        inp = inp.to(device)\n",
    "        w = w.to(device)\n",
    "        res = rmsnorm_part1_torch(inp, w)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        del w\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm_part2_torch(input, weight, ss):\n",
    "    return torch.multiply((1 / torch.sqrt(torch.as_tensor((ss/input.size(dim=0))) + 1)), torch.multiply(input, weight))\n",
    "\n",
    "def rmsnorm_part2_runner(weights, inputs, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i].flatten()\n",
    "        weight = weights[i].flatten()\n",
    "        ssum = np.sum(input * input)\n",
    "\n",
    "        inp = torch.from_numpy(input).to(dtype=torch.float32).to(cpu)\n",
    "        w = torch.from_numpy(weight).to(dtype=torch.float32).to(cpu)\n",
    "        ss = torch.tensor(ssum).to(dtype=torch.float32).to(cpu)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        inp = inp.to(device)\n",
    "        w = w.to(device)\n",
    "        ss = ss.to(device)\n",
    "        res = rmsnorm_part2_torch(inp, w, ss)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        del w\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu_torch(input, hidden_dim):\n",
    "    return torch.multiply(torch.divide(1, (torch.exp(0 - input[:hidden_dim]) + 1)), input[:hidden_dim])\n",
    "\n",
    "def silu_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i].flatten()\n",
    "        hd = len(input)\n",
    "\n",
    "        inp = torch.from_numpy(input).to(dtype=torch.float32).to(cpu)\n",
    "        hidden_dim = torch.tensor(hd).to(cpu)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        inp = inp.to(device)\n",
    "        hidden_dim = hidden_dim.to(device)\n",
    "        res = silu_torch(inp, hidden_dim)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part1_torch(input, max_pos):\n",
    "    return torch.max(input[:max_pos])\n",
    "\n",
    "def softmax_part1_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i].flatten()\n",
    "        mp = len(input)\n",
    "        \n",
    "        inp = torch.from_numpy(input).to(cpu)\n",
    "        max_pos = torch.tensor(mp).to(cpu)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        inp = inp.to(device)\n",
    "        max_pos = max_pos.to(device)\n",
    "        res = softmax_part1_torch(inp, max_pos)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part2_torch(input, max_pos, max_val):\n",
    "    return torch.exp(input[:max_pos] - max_val)\n",
    "\n",
    "def softmax_part2_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i].flatten()\n",
    "        mp = len(input)\n",
    "        \n",
    "        inp = torch.from_numpy(input).to(dtype=torch.float32).to(cpu)\n",
    "        max_pos = torch.tensor(mp).to(cpu)\n",
    "        max_val = torch.tensor(np.max(input[:mp])).to(dtype=torch.float32).to(cpu)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        inp = inp.to(device)\n",
    "        max_pos = max_pos.to(device)\n",
    "        max_val = max_val.to(device)\n",
    "        res = softmax_part2_torch(inp, max_pos, max_val)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part3_torch(output, max_pos):\n",
    "    return torch.sum(output[:max_pos])\n",
    "\n",
    "def softmax_part3_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i].flatten()\n",
    "        mp = len(input)\n",
    "        output = np.exp(input[:mp]-np.max(input[:mp]))\n",
    "        \n",
    "        outp = torch.from_numpy(output).to(dtype=torch.float32).to(cpu)\n",
    "        max_pos = torch.tensor(mp).to(cpu)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        outp = outp.to(device)\n",
    "        max_pos = max_pos.to(device)\n",
    "        res = softmax_part3_torch(outp, max_pos)\n",
    "        res = res.to(cpu)\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        del outp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part4_torch(unnormalized_output, max_pos, sum):\n",
    "    return unnormalized_output[:max_pos]/sum\n",
    "\n",
    "def softmax_part4_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        input = inputs[i].flatten()\n",
    "        mp = len(input)\n",
    "        output = np.exp(input[:mp]-np.max(input[:mp]))\n",
    "        s = np.sum(output[:mp])\n",
    "        \n",
    "        outp = torch.from_numpy(output).to(dtype=torch.float32).to(device)\n",
    "        max_pos = torch.tensor(mp).to(device)\n",
    "        sum = torch.tensor(s).to(dtype=torch.float32).to(device)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        softmax_part4_torch(outp, max_pos, sum)\n",
    "        end_time = time.perf_counter()\n",
    "        del outp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elewise_mul_pytorch_with_load\n",
      "558.834465360269ms +/- 64.03464096943887ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, w_input, None, elewise_mul_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_pytorch_with_load\n",
      "26.87900783494115ms +/- 1.230684701351786ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, aw_input, None, matmul_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiquery_attention_part1_pytorch_with_load\n",
      "1.1015573516488075ms +/- 0.1503313896433275ms\n"
     ]
    }
   ],
   "source": [
    "timer(k_weights, q_weights, None, multiquery_attention_part1_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "multiquery_attention_part2_pytorch_with_load\n",
      "0.5320644937455654ms +/- 0.46253850048238265ms\n"
     ]
    }
   ],
   "source": [
    "timer(k_weights, q_weights, None, multiquery_attention_part2_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsnorm_part1_pytorch_with_load\n",
      "330.09126214310527ms +/- 1.800282390717216ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, w_input, None, rmsnorm_part1_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsnorm_part2_pytorch_with_load\n",
      "1010.9781612642109ms +/- 12.108922029933726ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, w_input,None, rmsnorm_part2_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silu_pytorch_with_load\n",
      "875.178828695789ms +/- 3.14554028634092ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, None, None, silu_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part1_pytorch_with_load\n",
      "55.707092909142375ms +/- 2.5383555690753643ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part1_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part2_pytorch_with_load\n",
      "288.1737082730979ms +/- 1.0722158977785117ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part2_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part3_pytorch_with_load\n",
      "54.56754267215729ms +/- 0.11126528259816412ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part3_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part4_pytorch_with_load\n",
      "1.0392235592007637ms +/- 0.2029464233078125ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part4_runner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
