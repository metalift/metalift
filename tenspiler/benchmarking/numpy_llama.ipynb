{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = './vicuna_weight.h5'\n",
    "\n",
    "weights = []\n",
    "w_input = []\n",
    "attn_weights = []\n",
    "aw_input = []\n",
    "q_weights = []\n",
    "k_weights = []\n",
    "\n",
    "with h5py.File(weights_path, 'r') as weight_file:\n",
    "    for layer_name in weight_file:\n",
    "        w = np.squeeze(np.array(weight_file[layer_name])).astype(np.float32)\n",
    "        if \"model\" in layer_name and \"embed_tokens\" not in layer_name and \"layernorm\" not in layer_name:\n",
    "            weights.append(w)\n",
    "            w_input.append(rng.random(w.shape, dtype = np.float32))\n",
    "        if \"attn\" in layer_name:\n",
    "            attn_weights.append(w)\n",
    "            aw_input.append(rng.random(w.shape[1], dtype = np.float32))\n",
    "            if \"q_proj\" in layer_name:\n",
    "                q_weights.append(w)\n",
    "            if \"k_proj\" in layer_name:\n",
    "                k_weights.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(input1, input2, f, runner):\n",
    "    runs = 10\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        times.append(runner(input1, input2, f))\n",
    "    times = np.array(times)\n",
    "    print(f\"{runner.__name__[:-6]}np\")\n",
    "    print(f\"{np.average(times)}ms +/- {np.std(times)}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_part4_np(input1, input2, hidden_dim):\n",
    "    return (input1[:hidden_dim]) * (input2[:hidden_dim])\n",
    "\n",
    "def transformer_part4_runner(inputs1, inputs2, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs1)):\n",
    "        inp1 = inputs1[i].flatten()\n",
    "        inp2 = inputs2[i].flatten()\n",
    "        hidden_dim = len(inp1)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        transformer_part4_np(inp1, inp2, hidden_dim)\n",
    "        end_time = time.perf_counter()\n",
    "        del inp2\n",
    "        del inp1\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_np(weight, input):\n",
    "    return np.matmul(weight, input)\n",
    "\n",
    "def matmul_runner(weights, inputs, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        w = weights[i]\n",
    "        inp = inputs[i]\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        matmul_np(w, inp)\n",
    "        end_time = time.perf_counter()\n",
    "        del inp\n",
    "        del w\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_part1_np(token_position, head, head_size, key_cache_layer, q):\n",
    "    return (np.matmul(key_cache_layer[:token_position][:, (head) * (head_size):(head) * (head_size) + head_size], q[(head) * (head_size):(head) * (head_size) + head_size])) / (np.sqrt((head_size) * (1)))\n",
    "\n",
    "def transformer_part1_runner(k_matrixes, q_matrixes, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(k_matrixes)):\n",
    "        k_matrix = k_matrixes[i]\n",
    "        q_matrix = q_matrixes[i]\n",
    "        token_position = k_matrix.shape[0] - 1\n",
    "\n",
    "        num_head = 32\n",
    "        head = int(rng.integers(low=0, high=num_head))\n",
    "        head_size = k_matrix.shape[0] // num_head\n",
    "        \n",
    "        key_cache_layer = k_matrix\n",
    "        q = q_matrix.flatten()\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        transformer_part1_np(token_position, head, head_size, key_cache_layer, q)\n",
    "        end_time = time.perf_counter()\n",
    "        del key_cache_layer\n",
    "        del q\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_part2_np(token_position, head, head_size, key_cache_layer, attention):\n",
    "    return np.matmul(np.transpose(key_cache_layer[:(token_position) + (1)][:, (head) * (head_size):(head) * (head_size) + head_size]), attention[:(token_position) + (1)])\n",
    "\n",
    "def transformer_part2_runner(k_matrixes, q_matrixes, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(k_matrixes)):\n",
    "        k_matrix = k_matrixes[i]\n",
    "        q_matrix = q_matrixes[i]\n",
    "        token_position = k_matrix.shape[0] - 1\n",
    "\n",
    "        num_head = 32\n",
    "        head = int(rng.integers(low=0, high=num_head))\n",
    "        head_size = k_matrix.shape[0] // num_head\n",
    "        \n",
    "        key_cache_layer = k_matrix\n",
    "        q = q_matrix.flatten()\n",
    "\n",
    "        attention = transformer_part1_np(token_position, head, head_size, key_cache_layer, q)\n",
    "        attention = np.append(attention, np.array([0]))\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        transformer_part2_np(token_position, head, head_size, key_cache_layer, attention)\n",
    "        end_time = time.perf_counter()\n",
    "        del key_cache_layer\n",
    "        del attention\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm_part1_np(input, weight):\n",
    "    return np.sum((input) * (input))\n",
    "\n",
    "def rmsnorm_part1_runner(weights, inputs, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].flatten()\n",
    "        w = weights[i].flatten()\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        rmsnorm_part1_np(inp, w)\n",
    "        end_time = time.perf_counter()\n",
    "        del w\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsnorm_part2_np(input, weight, ss):\n",
    "    return ((1) / (np.sqrt(((ss) / (input.size)) + (1)))) * ((input) * (weight))\n",
    "\n",
    "def rmsnorm_part2_runner(weights, inputs, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].flatten()\n",
    "        w = weights[i].flatten()\n",
    "        ss = np.sum(inp * inp)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        rmsnorm_part2_np(inp, w, ss)\n",
    "        end_time = time.perf_counter()\n",
    "        del w\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_part3_np(input, hidden_dim):\n",
    "    return (input[:hidden_dim]) * ((1) / ((1) + (np.exp((0) - (input[:hidden_dim])))))\n",
    "\n",
    "def transformer_part3_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].flatten()\n",
    "        hidden_dim = len(inp)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        transformer_part3_np(inp, hidden_dim)\n",
    "        end_time = time.perf_counter()\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part1_np(input, max_pos):\n",
    "    return np.max(input[:max_pos])\n",
    "\n",
    "def softmax_part1_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].flatten()\n",
    "        max_pos = len(inp)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        softmax_part1_np(inp, max_pos)\n",
    "        end_time = time.perf_counter()\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part2_np(input, max_pos, max_val):\n",
    "    return np.exp((input[:max_pos]) - (max_val))\n",
    "\n",
    "def softmax_part2_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].flatten()\n",
    "        max_pos = len(inp)\n",
    "        \n",
    "        max_val = np.max(inp[:max_pos])\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        softmax_part2_np(inp, max_pos, max_val)\n",
    "        end_time = time.perf_counter()\n",
    "        del inp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part3_np(output, max_pos):\n",
    "    return np.sum(output[:max_pos])\n",
    "\n",
    "def softmax_part3_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].flatten()\n",
    "        max_pos = len(inp)\n",
    "        outp = np.exp(inp[:max_pos]-np.max(inp[:max_pos]))\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        softmax_part3_np(outp, max_pos)\n",
    "        end_time = time.perf_counter()\n",
    "        del outp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_part4_np(unnormalized_output, max_pos, sum):\n",
    "    return (unnormalized_output[:max_pos]) / (sum)\n",
    "\n",
    "def softmax_part4_runner(inputs, _, f=None):\n",
    "    total_time = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].flatten()\n",
    "        max_pos = len(inp)\n",
    "        outp = np.exp(inp[:max_pos]-np.max(inp[:max_pos]))\n",
    "        sum = np.sum(outp[:max_pos])\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        softmax_part4_np(outp, max_pos, sum)\n",
    "        end_time = time.perf_counter()\n",
    "        del outp\n",
    "        total_time += (end_time - start_time) * 1000\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elewise_mul_np\n",
      "772.6525171659887ms +/- 9.905152609367947ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, w_input, None, transformer_part4_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_np\n",
      "25.31529269181192ms +/- 0.6741979534251222ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, aw_input, None, matmul_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiquery_attention_part1_np\n",
      "0.5675445310771465ms +/- 0.4602312693214859ms\n"
     ]
    }
   ],
   "source": [
    "timer(k_weights, q_weights, None, transformer_part1_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiquery_attention_part2_np\n",
      "25.179899111390114ms +/- 1.2806089326884758ms\n"
     ]
    }
   ],
   "source": [
    "timer(k_weights, q_weights, None, transformer_part2_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsnorm_part1_np\n",
      "960.7194280717522ms +/- 1.4168013775142445ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, w_input, None, rmsnorm_part1_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsnorm_part2_np\n",
      "1432.575786486268ms +/- 1.6904189882236282ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, w_input,None, rmsnorm_part2_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silu_np\n",
      "3445.9609715268016ms +/- 4.010932109974055ms\n"
     ]
    }
   ],
   "source": [
    "timer(weights, None, None, transformer_part3_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part1_np\n",
      "70.29494065791368ms +/- 0.18874067353105572ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part1_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part2_np\n",
      "568.2537166867405ms +/- 0.7669734202011462ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part2_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part3_np\n",
      "90.50206989049911ms +/- 0.15729983246707896ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part3_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_part4_np\n",
      "305.4671247024089ms +/- 0.8038201821299202ms\n"
     ]
    }
   ],
   "source": [
    "timer(attn_weights, None, None, softmax_part4_runner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
